{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import face_detection\n",
    "import dlib\n",
    "from fer import FER\n",
    "import cv2\n",
    "import os\n",
    "from mtcnn import MTCNN\n",
    "import keras\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "WARNING:tensorflow:From <ipython-input-2-2e82a26757ae>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31-07-2020:01:59:56,445 WARNING  [deprecation.py:323] From <ipython-input-2-2e82a26757ae>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parkh\\Desktop\\[NOTA]facial_emotion_data\\train\\img\n",
      "['1.1.jpg', '1.image_8640454461499434291050.jpg', '1.jjCU.jpg', '10.10.jpg', '10.6fb6616b7bf78bdb_liu.jpg', '10.jj1tn.jpg', '100.smile-asians-2.jpg', '100.VD34944438_w640.jpg', '101.EliKiseopUKISS.jpg', '101.SSI_20161018150057.jpg']\n"
     ]
    }
   ],
   "source": [
    "# traing path \n",
    "cur_path = os.path.abspath(os.getcwd())\n",
    "img_dir = os.path.join(cur_path, \"train\\\\img\")\n",
    "print(img_dir)\n",
    "result_dir = os.path.join(cur_path, \"train\\\\img_result\")\n",
    "\n",
    "img_list = os.listdir(img_dir) \n",
    "print(img_list[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def run(start, end):  \n",
    "    # emotion detector\n",
    "    emotion_detector = FER()\n",
    "\n",
    "    # face detector\n",
    "    face_detector = face_detection.build_detector(\n",
    "      \"DSFDDetector\", confidence_threshold=.5, nms_iou_threshold=.3)\n",
    "    \n",
    "    cnt = 0\n",
    "    for img_name in img_list[start:end]:  \n",
    "            \n",
    "        # img\n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "\n",
    "        # BGR to RGB\n",
    "#         image = cv2.imread(img_path)[:, :, ::-1]\n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "        # face position\n",
    "        detections = face_detector.detect(image)\n",
    "\n",
    "        # show origin\n",
    "        print(\"<\",img_name, \">\")\n",
    "#         plt.imshow(image,aspect=\"auto\")\n",
    "#         plt.show()\n",
    "        if len(detections) == 0 :\n",
    "            cv2.imwrite(result_dir+\"\\\\failed\\\\\"+img_name,crop_image)  \n",
    "            print(\"detecting failed !!\")\n",
    "        else:\n",
    "#             print(\"detecting sucessed !!\")\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(len(detections)):\n",
    "            margin = 40\n",
    "            top = int(round(detections[i][1]))-margin\n",
    "            top = max(0, top)\n",
    "            bottom = int(round(detections[i][3]))+margin\n",
    "            left = int(round(detections[i][0]))-margin\n",
    "            left = max(0,left)\n",
    "            right =  int(round(detections[i][2]))+margin\n",
    "\n",
    "            crop_image = image[top:bottom, left:right]\n",
    "    #         print(crop_image)\n",
    "    #         plt.imshow(crop_image,aspect=\"auto\")\n",
    "    #         plt.show()\n",
    "    #         p = detections\n",
    "\n",
    "            # show window\n",
    "    #         win = dlib.image_window()\n",
    "    #         win.set_image(im)\n",
    "\n",
    "            # emotion \n",
    "\n",
    "            detect_res = emotion_detector.detect_emotions(crop_image)\n",
    "            save_name = img_name[:-4] +\"_\"+ str(i) +\".jpg\"\n",
    "            \n",
    "            if len(detect_res):\n",
    "                maximum = 0\n",
    "                top_emotion = ''\n",
    "                for emotion in detect_res[0]['emotions']:\n",
    "                    prop = detect_res[0]['emotions'][emotion]\n",
    "                    if maximum < prop:\n",
    "                        maximum = prop\n",
    "                        top_emotion = emotion\n",
    "                \n",
    "                if top_emotion == 'fear':\n",
    "                    top_emotion = 'surprise'\n",
    "                if top_emotion == 'diguste':\n",
    "                    top_emotion = 'anger'\n",
    "                \n",
    "                cv2.imwrite(result_dir+\"\\\\\"+top_emotion+\"\\\\\"+save_name,crop_image)\n",
    "                print(\"emotion is \", top_emotion)\n",
    "\n",
    "                del maximum\n",
    "                del top_emotion\n",
    "                \n",
    "            else:\n",
    "                print(\"emotion is failed\")\n",
    "                cv2.imwrite(result_dir+\"\\\\failed\\\\\"+save_name,crop_image)\n",
    "            \n",
    "            del crop_image\n",
    "            del detect_res\n",
    "               \n",
    "        del detections\n",
    "        del image\n",
    "        del img_path\n",
    "        \n",
    "    del emotion_detector\n",
    "    del face_detector\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:  1 total epoch: 1655\n",
      "\n",
      "epoch:  1\n",
      "< 11.6.jpg >\n",
      "emotion is  happy\n",
      "\n",
      "epoch:  2\n",
      "< 11.asian-man-with-surprised-shocked-face_39688-1297.jpg >\n",
      "emotion is  surprise\n",
      "\n",
      "epoch:  3\n",
      "< 11.happy-asian-family-stock-photo-2726146.jpg >\n",
      "emotion is  happy\n",
      "emotion is  happy\n",
      "emotion is  happy\n",
      "emotion is  happy\n",
      "\n",
      "epoch:  4\n",
      "< 11.VD57983418_w1280.jpg >\n",
      "emotion is failed\n",
      "\n",
      "epoch:  5\n",
      "< 110.1129815437.jpg >\n",
      "emotion is  surprise\n",
      "\n",
      "epoch:  6\n",
      "< 110.b2d812ad64e07994fe688c529274bb00.jpg >\n",
      "emotion is  sad\n",
      "\n",
      "epoch:  7\n",
      "< 111.280-jir0032-jj.jpg >\n",
      "emotion is  happy\n",
      "\n",
      "epoch:  8\n",
      "< 111.Asian-girls-shopping-515.jpg >\n",
      "emotion is failed\n",
      "emotion is failed\n",
      "emotion is failed\n",
      "emotion is failed\n",
      "emotion is failed\n",
      "emotion is failed\n",
      "emotion is failed\n",
      "emotion is failed\n",
      "\n",
      "epoch:  9\n",
      "< 111.crop.JPG >\n",
      "emotion is  happy\n",
      "\n",
      "epoch:  10\n",
      "< 111.crop2.JPG >\n",
      "emotion is  sad\n",
      "\n",
      "epoch:  11\n",
      "< 111.crop3.JPG >\n",
      "emotion is failed\n",
      "\n",
      "epoch:  12\n",
      "< 111.crop4.JPG >\n",
      "emotion is  sad\n",
      "\n",
      "epoch:  13\n",
      "< 111.crop5.JPG >\n",
      "emotion is  neutral\n",
      "\n",
      "epoch:  14\n",
      "< 111.maxresdefault.jpg >\n",
      "emotion is failed\n",
      "\n",
      "epoch:  15\n",
      "< 112.crop.JPG >\n",
      "emotion is  surprise\n",
      "\n",
      "epoch:  16\n",
      "< 112.crop2.JPG >\n",
      "emotion is  sad\n",
      "\n",
      "epoch:  17\n",
      "< 112.crop4.JPG >\n",
      "emotion is failed\n",
      "emotion is failed\n",
      "\n",
      "epoch:  18\n",
      "< 112.crop5.JPG >\n",
      "emotion is  sad\n",
      "\n",
      "epoch:  19\n",
      "< 112.IMG_28000.jpg >\n",
      "emotion is failed\n",
      "\n",
      "epoch:  20\n",
      "< 113.dsukmx45m835nw9bgn7p.jpg >\n",
      "emotion is  angry\n",
      "\n",
      "epoch:  21\n",
      "< 113.little-asian-boy-surprised-1432290.jpg >\n",
      "emotion is  sad\n",
      "\n",
      "epoch:  22\n",
      "< 114.crop.JPG >\n",
      "emotion is failed\n",
      "emotion is failed\n",
      "emotion is failed\n",
      "\n",
      "epoch:  23\n",
      "< 114.depositphotos_192482688-stock-video-happy-asian-women-using-smart.jpg >\n",
      "emotion is failed\n",
      "\n",
      "epoch:  24\n",
      "< 114.VD_crop.JPG >\n",
      "emotion is  sad\n",
      "\n",
      "epoch:  25\n",
      "< 115.58ef169516000027006582f2.jpg >\n",
      "emotion is  neutral\n",
      "\n",
      "epoch:  26\n",
      "< 115.92250_5758.jpg >\n",
      "emotion is failed\n",
      "\n",
      "epoch:  27\n",
      "< 115.PS19042501832.jpg >\n",
      "emotion is failed\n",
      "emotion is failed\n",
      "\n",
      "epoch:  28\n",
      "< 115.sevendeman150600056.jpg >\n",
      "emotion is failed\n",
      "\n",
      "epoch:  29\n",
      "< 116.5041510-asian-brunette-depth-of-field-dress-girl-long-hair-model-smile-woman.jpg >\n",
      "emotion is failed\n",
      "\n",
      "epoch:  30\n",
      "< 116.611211110012749457_1.jpg >\n",
      "emotion is  sad\n",
      "\n",
      "epoch:  31\n",
      "< 116.graphicstock-young-surprised-asian-woman-in-glasses-and-shirt-holding-the-clock-in-hands-isolated-gray-background_HLHfx_Qd3x_SB_PM.jpg >\n",
      "emotion is  surprise\n",
      "\n",
      "epoch:  32\n",
      "< 117.crop.JPG >\n",
      "emotion is failed\n",
      "\n",
      "epoch:  33\n",
      "< 117.inline_image_preview.jpg >\n",
      "emotion is  happy\n",
      "\n",
      "epoch:  34\n",
      "< 118.crop2.JPG >\n",
      "emotion is  sad\n",
      "emotion is failed\n",
      "\n",
      "epoch:  35\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.32 GiB (GPU 0; 8.00 GiB total capacity; 4.26 GiB already allocated; 807.19 MiB free; 5.12 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-dc8fa42b0816>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-87-83a677c049da>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(start, end)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# face position\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mdetections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# show origin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nota\\lib\\site-packages\\face_detection-0.1.4-py3.7.egg\\face_detection\\base.py\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(self, image, shrink)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \"\"\"\n\u001b[0;32m     49\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatched_detect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshrink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nota\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_no_grad\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nota\\lib\\site-packages\\face_detection-0.1.4-py3.7.egg\\face_detection\\base.py\u001b[0m in \u001b[0;36mbatched_detect\u001b[1;34m(self, image, shrink)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pre_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshrink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batched_detect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[0mboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mscale_boxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbox\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_detections\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nota\\lib\\site-packages\\face_detection-0.1.4-py3.7.egg\\face_detection\\base.py\u001b[0m in \u001b[0;36m_batched_detect\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_batched_detect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0mboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_detect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[0mboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_boxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nota\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_no_grad\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nota\\lib\\site-packages\\face_detection-0.1.4-py3.7.egg\\face_detection\\dsfd\\detect.py\u001b[0m in \u001b[0;36m_detect\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         boxes = self.net(\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfidence_threshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnms_iou_threshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         )\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nota\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nota\\lib\\site-packages\\face_detection-0.1.4-py3.7.egg\\face_detection\\dsfd\\face_ssd.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, confidence_threshold, nms_threshold)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         sources = [\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpm3_3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv3_3_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpm4_3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv4_3_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpm5_3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv5_3_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nota\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nota\\lib\\site-packages\\face_detection-0.1.4-py3.7.egg\\face_detection\\dsfd\\face_ssd.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mx2_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpm4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mx3_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpm5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx1_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx3_1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.32 GiB (GPU 0; 8.00 GiB total capacity; 4.26 GiB already allocated; 807.19 MiB free; 5.12 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "epoch = 0\n",
    "start = 33\n",
    "end = len(img_list)\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"batch size: \", batch_size, \"total epoch:\", end-start)\n",
    "for i in range(start,end,batch_size):\n",
    "    epoch += batch_size\n",
    "    print(\"\\nepoch: \", epoch)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    run(i,i+1)\n",
    "    \n",
    "end_time = datetime.time()\n",
    "print(\"elapsed time: \", end_time-start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nota] *",
   "language": "python",
   "name": "conda-env-nota-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
